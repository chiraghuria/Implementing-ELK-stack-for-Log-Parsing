input {

# File input for sample logs
    file {
        path => "C:\abc\log\logstash-2.4.0\log-input\sample-logs.log"
        start_position => beginning  
        # sincedb_path => "NUL"
		ignore_older => 0
		codec => multiline {
          pattern => "^%{TIMESTAMP_ISO8601} "
          negate => true
          what => previous
		}
    }
# Optional command line input plugin
	# stdin { }
}

filter {

# Removing the trailing carriage return
	mutate {
      gsub => [ "message", "\r", "" ]
    }
	
# Grokking of the logs
	grok {
	  match => {"message" => "%{YEAR:Year}-%{MONTHNUM:Month}-%{MONTHDAY:Date} %{TIME:Time} %{LOGLEVEL:Severity}%{SPACE}%{WORD:Class}%{SPACE}-%{SPACE}%{GREEDYDATA:Description}"}
	  add_field => {"Log Type" => "%{Severity}"}
	}
	
# Typecasting Severity to lowercase for dynamic indexing in Elasticsearch
	ruby {
	  code => "event['Log Type'] = event['Log Type'].to_s.force_encoding('ISO-8859-1').downcase"
	}
# Matching current timestamp with the logged event timestamp
	mutate {
	  add_field => {"timestamp" => "%{Year}-%{Month}-%{Date} %{Time}"}
	}
	date{
	  match=>["timestamp","yyyy-MM-dd HH:mm:ss"]
	  timezone => "UTC"
	}
	mutate {
	  remove_field => ["timestamp","Year","Month","Date", "Time"]
	}
	
# Adding a fingerprint for unique identification of every index
	fingerprint {
      method => "UUID"
    }
}

output {

# Rubydebug format output	
    file {
		path => "C:\abc\log\logstash-2.4.0\log-input\parsed-logs.rubydebug"
		codec => "rubydebug"
    }
	
# CSV format output
	csv {
	  path => "C:\abc\log\logstash-2.4.0\log-input\parsed-logs.csv"
	  fields => ["@timestamp","Severity","Class","Description","host"]
	}
	
# JSON format output
	file {
		path => "C:\abc\log\logstash-2.4.0\log-input\parsed-logs.json"
		codec => "json_lines"
    }
# Commandline output	
	stdout { codec => "rubydebug" }

# Elasticsearch outputs
	elasticsearch {
	  hosts => ["localhost:9200"]
	  index => "%{Log Type}-logs"
	}
	elasticsearch {
	  hosts => ["localhost:9200"]
	  index => "log-%{+YYYY.MM.dd}"
	}
}
